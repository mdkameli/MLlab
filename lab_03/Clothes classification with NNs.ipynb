{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this notebook we are going to explore the Neural Networks for image classification. We are going to use the same dataset of the SVM notebook: Fashion MNIST (https://pravarmahajan.github.io/fashion/), a dataset of small images of clothes and accessories.\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the required packages\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load Fashion MNIST dataset from disk\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,offset=16).reshape(len(labels), 784)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO \n",
    "Place your ID (\"numero di matricola\") that will be used as seed for random generator. You can try to change the seed to see the impact of the randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1165385\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the MNIST dataset and let's normalize the features so that each value is in [0,1]\n",
    "X, y = load_mnist(\"data\")\n",
    "# rescale the data\n",
    "X = X / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test. Make sure that each label is present at least 10 times\n",
    "in training frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random permute the data and split into training and test taking the first 500\n",
    "#data samples as training and the rests as test\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 500\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %i\"%labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try the plotting function\n",
    "plot_input(X_train,y_train,10)\n",
    "plot_input(X_test,y_test,100)\n",
    "plot_input(X_test,y_test,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 1\n",
    "\n",
    "Now use a Feed-forward Neural Network for prediction. Use the multi-layer perceptron classifier, with the following parameters: max_iter=300, alpha=1e-4, solver='sgd', tol=1e-4, learning_rate_init=.1, random_state=ID (this last parameter ensures the run is the same even if you run it more than once). The alpha parameter is the regularization term.\n",
    "\n",
    "Then, using the default activation function, pick four or five architectures to consider, with different numbers of hidden layers and different sizes. It is not necessary to create huge neural networks, you can limit to 3 layers and, for each layer, its maximum size can be of 100. Evaluate the architectures you chose using the GridSearchCV with cv=5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'hidden_layer_sizes': [(10,), (50,), (10,10,), (50,50,),(50,50,50,),(100,80,50,),(50,80,100,)]}\n",
    "\n",
    "mlp = MLPClassifier(max_iter=300, alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=ID,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "clf = GridSearchCV(mlp, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR NN\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "mean = max(clf.cv_results_['mean_test_score'])\n",
    "params = clf.best_params_\n",
    "print(\"%0.3f \"% mean)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "What do you observe for different architectures and their scores? How the nummber of layers and their sizes affect the performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer (1):\n",
    "the results shows that for the small number of samples if we increase the layer to 3 the score would decrease to due the fact that there is not enough nodes to connect and make a meaningful structure that leads us to better answer. In short, a medium structure for midium range of samples has the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 2\n",
    "\n",
    "Now get training and test error for a NN with best parameters from above. Use verbose=True\n",
    "in input so to see how loss changes in iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and test error for the best NN model from CV\n",
    "\n",
    "#best_param = clf.best_params_\n",
    "best_mlp = MLPClassifier(max_iter=300, alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=ID,\n",
    "                    learning_rate_init=.1,hidden_layer_sizes = clf.best_params_['hidden_layer_sizes'])\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "training_error = 1. - best_mlp.score(X_train,y_train)\n",
    "test_error = 1. - best_mlp.score(X_test,y_test)\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data \n",
    "Now let's do the same but using 10000 (or less if it takes too long on your machine) data points for training. Use the same NN architectures as before, but you can try more if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 3000\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "print(\"Labels and frequencies in training dataset: \")\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 3\n",
    "\n",
    "Now train the NNs with the added data points. Feel free to try more different architectures than before if you want, or less if it takes too much time. We suggest that you use 'verbose=True' so have an idea of how long it takes to run 1 iteration (eventually reduce also the number of iterations to 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for NN we try the same architectures as before\n",
    "parameters = {'hidden_layer_sizes': [(10,), (50,),(100,),(10,10,),(50,50,),(80,80,),(100,80,50,),(50,80,100,)]}\n",
    "\n",
    "mlp_large = MLPClassifier(max_iter=50, alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=ID,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp_large_CV = GridSearchCV(mlp_large, parameters, cv=5)\n",
    "mlp_large_CV.fit(X_train, y_train)\n",
    "\n",
    "print ('\\nRESULTS FOR NN\\n')\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(mlp_large_CV.best_params_)\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(mlp_large_CV.best_score_)\n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "print(mlp_large_CV.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2\n",
    "Describe your architecture choices and the results you observe with respect to the layers and sizes used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer (2):\n",
    "First of all because of my lap-top low performance I had to decrease the number of input samples to 3000. Second of all i expected that by increasing the number of samples, more layer with higher complexity in comparison with the previous question should be the result! but, in contrst the number of layer decreased to one and with complexity of 50. the reasons could be randomness nor number of iteration which leads not to converge situation for samples. It maybe possible by increasing the number of iteration the complexity structure would increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 4\n",
    "\n",
    "Get the train and test error for the best NN you obtained with 3000 points. This time you can run for 100 iterations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get training and test error for the best NN model from CV\n",
    "\n",
    "best_mlp_large = MLPClassifier(max_iter=100, alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=ID,\n",
    "                    learning_rate_init=.1,hidden_layer_sizes = mlp_large_CV.best_params_['hidden_layer_sizes'])\n",
    "\n",
    "best_mlp_large.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "training_error = 1. - best_mlp_large.score(X_train,y_train)\n",
    "test_error = 1. - best_mlp_large.score(X_test,y_test)\n",
    "\n",
    "print ('RESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3\n",
    "\n",
    "Compare the train and test error you got with a large number of samples with the best one you obtained with only 500 data points. Are the architectures the same or do they differ? What about the errors you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer (3):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found for 500 samples:\")\n",
    "print(clf.best_params_)\n",
    "print(\"\\nBest parameters set found for 3000 samples:\")\n",
    "print(mlp_large_CV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned above the artitectures are different beside the expectation but the error decreased for the new structure and oncreasing the number of input samples as we expected also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 5\n",
    "\n",
    "Plot a digit that was missclassified by NN with m=500 training data points and it is now instead correctly classified by NN with m=10000 training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_prediction = clf.predict(X_test)\n",
    "large_NN_prediction = best_mlp_large.predict(X_test)\n",
    "\n",
    "zip_func = zip(NN_prediction,large_NN_prediction)\n",
    "for i,j in enumerate(zip_func):\n",
    "    if ((j[0] != y_test[i]) & (j[1] == y_test[i] )):\n",
    "        break\n",
    "print(\"NN_prediction\",NN_prediction[i])\n",
    "print(\"large_NN_prediction\",large_NN_prediction[i])\n",
    "plot_input(X_test,y_test,i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the weigths of the multi-layer perceptron classifier, for the best NN we get with 500 data points and with 3000 data points. Notice that the code assumes that the NNs are called \"mlp\" and \"best_mlp_large\" , you could need to replace with your variable names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights with 500 data points:\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "vmin, vmax = best_mlp.coefs_[0].min(), best_mlp.coefs_[0].max()\n",
    "for coef, ax in zip(best_mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin, vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Weights with 3000 data points:\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "vmin, vmax = best_mlp_large.coefs_[0].min(), best_mlp_large.coefs_[0].max()\n",
    "for coef, ax in zip(best_mlp_large.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin, vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 4\n",
    "\n",
    "Describe what do you observe by looking at the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer (4):\n",
    "First of all I had to mention because of the low performance I decreased the input sample numbers to 3000 which cause that the difference is not so obvious but if we get more cautious we could find that the accuracy increased and the number of the coefficient stand for best performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 7\n",
    "\n",
    "Report the best SVM model and its parameters, you found in the last notebook. Fit it on a few data points and compute its training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_training = 700\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:2*m_training]\n",
    "y_train, y_test = y[:m_training], y[m_training:2*m_training]\n",
    "\n",
    "# best parameters found in the SVM notebook\n",
    "# Create the SVM and perform the fit\n",
    "\n",
    "best_svc = SVC(kernel = 'rbf')\n",
    "parameter = {'C': [10], 'gamma': [0.01]}\n",
    "best_SVM = GridSearchCV(best_svc, parameter, cv=5)\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR SVM\\n\\n')\n",
    "\n",
    "SVM_training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "\n",
    "print(\"Training score SVM:\")\n",
    "print(SVM_training_error)\n",
    "\n",
    "SVM_test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "print(\"\\nTest score SVM:\")\n",
    "print(SVM_test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## QUESTION 5\n",
    "Compare the results of SVM and of NN. Which one would you preferer? Which are its tradeoffs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Answer (5):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First of all the SVM is way faster than NN for larger number of samples, but the accuracy of NN networks are better than the SVM due to error just for the large number of input samples. I think it would be a tradeoff between acuuracy, performance, speed, complexity and etc. always find the best way and the best algorithm would specifically depends on the input data and the ristriction of the hardware and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
